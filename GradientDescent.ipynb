{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent and critical points in DNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finding and Characterizing the Critical Points on the Loss Surface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Start\n",
    "\n",
    "http://ruder.io/optimizing-gradient-descent/\n",
    "\n",
    "A bit of background: Gradient, Critical points, Hessian and its Eigenvalues. Probability of having all eigenvalues positiv is increasingly small. \n",
    "\n",
    "However Deep Nets must have a natural multiplicity, (that I never read about): The product of permutation sizes of all hidden layers.\n",
    "\n",
    "$$\\Pi_{n \\in N_h} \\frac{1}{n (n - 1)}$$\n",
    "\n",
    "\n",
    "I peek into the scientific literature:\n",
    "\n",
    "-----\n",
    "\n",
    "An approachable, good read on the role of saddle points (unpublished)\n",
    "[Are Saddles Good Enough for Deep Learning](https://arxiv.org/abs/1706.02052), Sankar et al. arXiv:1706.02052 [stat.ML]\n",
    "\n",
    "-----\n",
    "\n",
    "Choromanska, A., Henaff, M., Mathieu, M., Arous, G. B., & LeCun, Y. (2015). The loss surfaces of multilayer networks. Journal of Machine Learning Research, 38, 192-204.\n",
    "\n",
    "Empirically verify that\n",
    "\n",
    "- For large-size networks, most local minima are equivalent and yield similar performance on a test set.\n",
    "- The probability of finding a “bad” (high value) local minimum is non-zero for small-size networks and decreases quickly with network size.\n",
    "- Struggling to find the global minimum on the training set (as opposed to one of the many good local ones) is not useful in practice and may lead to overfitting.\n",
    "\n",
    "----\n",
    "\n",
    "Dauphin, Y., Pascanu, R., Gülcehre, C¸ ., Cho, K., Ganguli, S., and Bengio, Y. (2014). Identifying and attacking the saddle point problem in high-dimensional non-convex optimization. In NIPS.\n",
    "\n",
    "Claim that it's not local minima but rather high error saddle points that can keep the network from converging well. The devise a strategy called \"Saddle-free Newton method\" to *attack* the saddles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
